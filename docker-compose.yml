# Maintainer: Willis Chen <misweyu2007@gmail.com>
services:
  # 向量資料庫：Qdrant (負責資安情資儲存與檢索)
  cisco-foundation-sec-8b-qdrant:
    image: qdrant/qdrant:v1.17.0 # 建議使用具體版本號以確保校務系統穩定
    container_name: cisco-foundation-sec-8b-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:6333/healthz" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: unless-stopped

  # 資安應用程式：Chainlit + Foundation-Sec-8B (GPU 加速)
  security-app:
    build: .
    container_name: security-app-gpu
    restart: unless-stopped
    volumes:
      # 將整個目錄掛載會覆蓋掉 Image 中已下載的 /app/models
      # 改為只掛載會頻繁修改的程式碼檔案，確保 Image 內建的模型檔案保留
      - ./:/app
    environment:
      # NVIDIA 容器配置
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # 應用程式參數
      - QDRANT_URL=http://cisco-foundation-sec-8b-qdrant:6333
      - MODEL_SEC_PATH=/app/models/foundation-sec-8b-q4_k_m.gguf
      - MODEL_LLAMA3_PATH=/app/models/llama-3-taiwan-8b-instruct-q4_k_m.gguf

      # RTX 2060 6GB VRAM 分配策略 (Q4_K_M 8B 各有 32 層，每層約 140MB)：
      # - 兩個模型全部上 GPU 需要約 9GB → 遠超 6GB 上限
      # - Llama-3-Taiwan (分類+翻譯)：10/32 層上 GPU → ~1.4GB（速度優先，品質次之）
      # - Foundation-Sec (資安分析)：20/32 層上 GPU → ~2.8GB（分析品質優先）
      # - 合計約 4.2GB，留 ~1.8GB 給 KV cache 與 CUDA context overhead
      # ⚠️ 若仍 OOM，請將兩個數值各降低 5 再試
      - N_GPU_LAYERS_LLAMA3=10
      - N_GPU_LAYERS_SEC=20

      # 針對 RTX 2060 6GB 的 Python/CUDA 優化 (防止記憶體碎片化)
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

    # 硬體資源分配：將 GPU 穿透進容器
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # 執行指令 (-w 參數支援熱更新，方便開發)
    command: chainlit run cisco_security_chainlit.py --host 0.0.0.0
    ports:
      - "8000:8000"

networks:
  default:
    name: cisco_sec_net
