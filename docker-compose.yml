# Maintainer: Willis Chen <misweyu2007@gmail.com>
services:
  # 向量資料庫：Qdrant (負責資安情資儲存與檢索)
  cisco-foundation-sec-8b-qdrant:
    image: qdrant/qdrant:v1.17.0 # 建議使用具體版本號以確保校務系統穩定
    container_name: cisco-foundation-sec-8b-qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_storage:/qdrant/storage
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:6333/healthz" ]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 15s
    restart: unless-stopped

  # 資安應用程式：Chainlit + Foundation-Sec-8B (GPU 加速)
  security-app:
    working_dir: /app
    build: .
    container_name: security-app-gpu
    restart: unless-stopped
    environment:
      # NVIDIA 容器配置
      - NVIDIA_VISIBLE_DEVICES=all
      - NVIDIA_DRIVER_CAPABILITIES=compute,utility

      # 應用程式參數
      - QDRANT_URL=http://cisco-foundation-sec-8b-qdrant:6333
      - MODEL_SEC_PATH=/app/models/foundation-sec-8b-q4_k_m.gguf
      - MODEL_LLAMA3_PATH=/app/models/llama-3-taiwan-8b-instruct-q4_k_m.gguf

      # RTX 2060 6GB VRAM 分配策略 (Q4_K_M 8B 各有 32 層，每層約 140MB)：
      # - Llama-3-Taiwan (分類+翻譯)：10/32 層上 GPU → ~1.26GB
      # - Foundation-Sec (資安分析)：15/32 層上 GPU → ~1.87GB
      # - 兩個 KV cache：~0.48GB，compute buffer：~1.34GB
      # - 合計約 4.95GB，留 ~1GB 給 CUDA context overhead 與碎片
      # ⚠️ 若仍 OOM，請將 N_GPU_LAYERS_SEC 再降 5
      - N_GPU_LAYERS_LLAMA3=5
      - N_GPU_LAYERS_SEC=10

      # 針對 RTX 2060 6GB 的 Python/CUDA 優化 (防止記憶體碎片化)
      - PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True

    # 硬體資源分配：將 GPU 穿透進容器
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [ gpu ]

    # 執行指令 (-w 參數支援熱更新，方便開發)
    command: chainlit run cisco_security_chainlit.py --host 0.0.0.0
    ports:
      - "8000:8000"

networks:
  default:
    name: cisco_sec_net
